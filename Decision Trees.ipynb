{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common imports\n",
    "import numpy as np\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=2,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=42,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data[:, 2:] #petal length and width\n",
    "y = iris.target\n",
    "\n",
    "tree_clf = DecisionTreeClassifier(max_depth=2, random_state=42)\n",
    "tree_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the many qualities of Decision Trees is that they require\n",
    "very little data preparation. In particular, they don’t require feature scaling or centering at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First install graphivz on your PC and then type in your conda prompt \"conda install graphviz\"\n",
    "import graphviz\n",
    "import os\n",
    "os.chdir('C:/Users/HP/Anaconda3/Library/bin/graphviz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\r\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n",
       "<!-- Generated by graphviz version 2.38.0 (20140413.2041)\r\n",
       " -->\r\n",
       "<!-- Title: Tree Pages: 1 -->\r\n",
       "<svg width=\"341pt\" height=\"269pt\"\r\n",
       " viewBox=\"0.00 0.00 341.00 269.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 265)\">\r\n",
       "<title>Tree</title>\r\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-265 337,-265 337,4 -4,4\"/>\r\n",
       "<!-- 0 -->\r\n",
       "<g id=\"node1\" class=\"node\"><title>0</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M188,-261C188,-261 75,-261 75,-261 69,-261 63,-255 63,-249 63,-249 63,-205 63,-205 63,-199 69,-193 75,-193 75,-193 188,-193 188,-193 194,-193 200,-199 200,-205 200,-205 200,-249 200,-249 200,-255 194,-261 188,-261\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"131.5\" y=\"-245.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">X[0] &lt;= 2.45</text>\r\n",
       "<text text-anchor=\"middle\" x=\"131.5\" y=\"-230.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.667</text>\r\n",
       "<text text-anchor=\"middle\" x=\"131.5\" y=\"-215.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 150</text>\r\n",
       "<text text-anchor=\"middle\" x=\"131.5\" y=\"-200.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [50, 50, 50]</text>\r\n",
       "</g>\r\n",
       "<!-- 1 -->\r\n",
       "<g id=\"node2\" class=\"node\"><title>1</title>\r\n",
       "<path fill=\"#e58139\" stroke=\"black\" d=\"M109,-149.5C109,-149.5 12,-149.5 12,-149.5 6,-149.5 0,-143.5 0,-137.5 0,-137.5 0,-108.5 0,-108.5 0,-102.5 6,-96.5 12,-96.5 12,-96.5 109,-96.5 109,-96.5 115,-96.5 121,-102.5 121,-108.5 121,-108.5 121,-137.5 121,-137.5 121,-143.5 115,-149.5 109,-149.5\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"60.5\" y=\"-134.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\r\n",
       "<text text-anchor=\"middle\" x=\"60.5\" y=\"-119.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 50</text>\r\n",
       "<text text-anchor=\"middle\" x=\"60.5\" y=\"-104.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [50, 0, 0]</text>\r\n",
       "</g>\r\n",
       "<!-- 0&#45;&gt;1 -->\r\n",
       "<g id=\"edge1\" class=\"edge\"><title>0&#45;&gt;1</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M108.448,-192.884C100.64,-181.666 91.9113,-169.126 84.0636,-157.852\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"86.8495,-155.728 78.2639,-149.52 81.1043,-159.727 86.8495,-155.728\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"73.8543\" y=\"-170.428\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">True</text>\r\n",
       "</g>\r\n",
       "<!-- 2 -->\r\n",
       "<g id=\"node3\" class=\"node\"><title>2</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M256,-157C256,-157 151,-157 151,-157 145,-157 139,-151 139,-145 139,-145 139,-101 139,-101 139,-95 145,-89 151,-89 151,-89 256,-89 256,-89 262,-89 268,-95 268,-101 268,-101 268,-145 268,-145 268,-151 262,-157 256,-157\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"203.5\" y=\"-141.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">X[1] &lt;= 1.75</text>\r\n",
       "<text text-anchor=\"middle\" x=\"203.5\" y=\"-126.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.5</text>\r\n",
       "<text text-anchor=\"middle\" x=\"203.5\" y=\"-111.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 100</text>\r\n",
       "<text text-anchor=\"middle\" x=\"203.5\" y=\"-96.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 50, 50]</text>\r\n",
       "</g>\r\n",
       "<!-- 0&#45;&gt;2 -->\r\n",
       "<g id=\"edge2\" class=\"edge\"><title>0&#45;&gt;2</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M154.876,-192.884C161.038,-184.154 167.765,-174.625 174.196,-165.514\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"177.087,-167.487 179.995,-157.299 171.368,-163.451 177.087,-167.487\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"184.242\" y=\"-178.236\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">False</text>\r\n",
       "</g>\r\n",
       "<!-- 3 -->\r\n",
       "<g id=\"node4\" class=\"node\"><title>3</title>\r\n",
       "<path fill=\"#39e581\" fill-opacity=\"0.898039\" stroke=\"black\" d=\"M182,-53C182,-53 85,-53 85,-53 79,-53 73,-47 73,-41 73,-41 73,-12 73,-12 73,-6 79,-0 85,-0 85,-0 182,-0 182,-0 188,-0 194,-6 194,-12 194,-12 194,-41 194,-41 194,-47 188,-53 182,-53\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"133.5\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.168</text>\r\n",
       "<text text-anchor=\"middle\" x=\"133.5\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 54</text>\r\n",
       "<text text-anchor=\"middle\" x=\"133.5\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 49, 5]</text>\r\n",
       "</g>\r\n",
       "<!-- 2&#45;&gt;3 -->\r\n",
       "<g id=\"edge3\" class=\"edge\"><title>2&#45;&gt;3</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M179.017,-88.9485C172.444,-80.0749 165.326,-70.4648 158.714,-61.5388\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"161.329,-59.189 152.564,-53.2367 155.704,-63.3556 161.329,-59.189\"/>\r\n",
       "</g>\r\n",
       "<!-- 4 -->\r\n",
       "<g id=\"node5\" class=\"node\"><title>4</title>\r\n",
       "<path fill=\"#8139e5\" fill-opacity=\"0.976471\" stroke=\"black\" d=\"M321,-53C321,-53 224,-53 224,-53 218,-53 212,-47 212,-41 212,-41 212,-12 212,-12 212,-6 218,-0 224,-0 224,-0 321,-0 321,-0 327,-0 333,-6 333,-12 333,-12 333,-41 333,-41 333,-47 327,-53 321,-53\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"272.5\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.043</text>\r\n",
       "<text text-anchor=\"middle\" x=\"272.5\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 46</text>\r\n",
       "<text text-anchor=\"middle\" x=\"272.5\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 1, 45]</text>\r\n",
       "</g>\r\n",
       "<!-- 2&#45;&gt;4 -->\r\n",
       "<g id=\"edge4\" class=\"edge\"><title>2&#45;&gt;4</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M227.633,-88.9485C234.112,-80.0749 241.129,-70.4648 247.646,-61.5388\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"250.638,-63.3769 253.708,-53.2367 244.984,-59.249 250.638,-63.3769\"/>\r\n",
       "</g>\r\n",
       "</g>\r\n",
       "</svg>\r\n"
      ],
      "text/plain": [
       "<graphviz.files.Source at 0x19ea30a4208>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "export_graphviz(tree_clf,\n",
    "                out_file='tree.dot',\n",
    "                filled=True,\n",
    "                rounded=True)\n",
    "\n",
    "with open(\"tree.dot\") as f:\n",
    "    dot_graph=f.read()\n",
    "graphviz.Source(dot_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Node’s gini attribute measures its impurity: a node is “pure” (gini=0) if all training instances it applies to belong to the same\n",
    "class.\n",
    "\n",
    "$G_{i} = 1- \\sum_{k=1}^{n} p_{i,k}^2$\n",
    "\n",
    "where $p_{i,k}$ is the ratio of class $k$ instances among the training instances in the $i^{th}$ node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The CART training algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-Learn uses the Classification And Regression Tree (CART) algorithm to train Decision Trees. The idea is really quite simple: the algorithm first splits the training set in two subsets using a single feature $k$ and a threshold $t_k$ (e.g., “petal length ≤ 2.45 cm”). How does it choose $k$ and $t_k$? It searches for the pair ($k$, $t_k$) that produces the purest subsets (weighted by their size).\n",
    "\n",
    "$ J(k, t_k) = \\frac{m_{left}}{m}G_{left} + \\frac{m_{right}}{m}G_{right}$\n",
    "\n",
    "where:\n",
    "\n",
    "- $G_{left/right}$ measures the impurity of the left/right subset\n",
    "- $m_{left/right}$ is the number of instances in the left/right subset\n",
    "\n",
    "Once it has successfully split the training set in two, it splits the subsets using the same logic, then the sub-subsets and so on, recursively. It stops recursing once it reaches the maximum depth (defined by the max_depth hyperparameter), or if it cannot find a split that will reduce impurity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gini Impurity or Entropy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the Gini impurity measure is used, but you can select the entropy impurity measure instead by setting the criterion hyperparameter to \"entropy\". In Machine Learning, it is frequently used as an impurity measure: a set’s entropy is zero when it contains instances of only one class.\n",
    "\n",
    "$H_{i} = - \\sum_{k=1}^{n} p_{i,k} log(p_{i,k})$\n",
    "\n",
    "So should you use Gini impurity or entropy? The truth is, most of the time it does not make a big difference: they lead to similar trees. Gini impurity is slightly faster to compute, so it is a good default. However, when they differ, Gini impurity tends to isolate the most frequent class in its own branch of the tree, while entropy tends to\n",
    "produce slightly more balanced trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Such a model is often called a nonparametric model, not because it does not have any parameters (it often has a lot) but because the\n",
    "number of parameters is not determined prior to training, so the model structure is free to stick closely to the data. In contrast, a parametric model such as a linear model has a predetermined number of parameters, so its degree of freedom is limited, reducing the risk of overfitting (but increasing the risk of underfitting).\n",
    "\n",
    "To avoid overfitting the training data, you need to restrict the Decision Tree’s freedom during training. As you know by now, this is called regularization. The regularization hyperparameters depend on the algorithm used, but generally you can at least restrict\n",
    "the maximum depth of the Decision Tree. In Scikit-Learn, this is controlled by the max_depth hyperparameter (the default value is None, which means unlimited). Reducing max_depth will regularize the model and thus reduce the risk of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other algorithms work by first training the Decision Tree without\n",
    "restrictions, then pruning (deleting) unnecessary nodes. A node\n",
    "whose children are all leaf nodes is considered unnecessary if the\n",
    "purity improvement it provides is not statistically significant. Standard statistical tests, such as the χ2 test, are used to estimate the probability that the improvement is purely the result of chance\n",
    "(which is called the null hypothesis). If this probability, called the pvalue, is higher than a given threshold (typically 5%, controlled by a hyperparameter), then the node is considered unnecessary and its children are deleted. The pruning continues until all unnecessary nodes have been pruned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However they do have a few limitations. First, as you may have noticed, Decision Trees love orthogonal decision boundaries (all splits are perpendicular to an axis), which makes them sensitive to training set rotation.\n",
    "\n",
    "One way to limit this problem is to use PCA.\n",
    "\n",
    "More generally, the main issue with Decision Trees is that they are very sensitive to small variations in the training data.\n",
    "\n",
    "Random Forests can limit this instability by averaging predictions over many trees."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
